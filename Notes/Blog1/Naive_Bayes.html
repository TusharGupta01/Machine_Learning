<!doctype html><html><head><meta charset="utf-8"><style>
      /* Separated from document_view.css to enable use in downloadable
   export files. #cutAndCopyBucket is included for every rule because
   we wwant cut/copied items to be formatted the same way as they
   would be when exported. */

.formattedExport,
#cutAndCopyBucket {
    font-family: 'Helvetica Neue', Arial, Sans-serif;
    color: #333;
    font-size: 13px;
    line-height: 17px;
    /* used to set white-space: pre-wrap here, but that messes up GMail's
       formatting of the export output, so moving that to the name and
       notes separately. */
}

.formattedExport .name, .formattedExport .note,
#cutAndCopyBucket .name, #cutAndCopyBucket .note {
    white-space: pre-wrap;
}

.formattedExport ul,
#cutAndCopyBucket ul{
    list-style: disc;
    /* Needed to reset browser defaults. */
    margin: 0;
    padding: 0;
}

.formattedExport li,
#cutAndCopyBucket li {
    margin: 4px 0 4px 20px;
    /* Needed to reset browser defaults. */
    padding: 0;
}

.formattedExport > .name,
#cutAndCopyBucket > .name {
    font-size: 16px;
    line-height: 21px;
}

.formattedExport > .note,
#cutAndCopyBucket > .note {
    font-size: 13px;
    line-height: 17px;
}

.formattedExport > ul,
#cutAndCopyBucket > ul {
    margin-top: 15px;
}

.formattedExport .name.done,
#cutAndCopyBucket .name.done {
    text-decoration:line-through;
    color:#999;
}

.formattedExport .note,
#cutAndCopyBucket .note {
    font-size: 12px;
    color:#666;
}

      </style></head><body class="formattedExport"><ul><li><span class="name">Machine Learning </span><ul><li><span class="name">Naive Bayes Classifier </span><ul><li><span class="name">The Naive Bayes classifier is a simple probabilistic classifier which is based on Bayes theorem with strong and naive independence assumption. </span><ul><li><span class="name">Naive Bayes comes handy since it can be trained very quickly. </span></li><li><span class="name">Multinomial Naive Bayes is used when the multiple occurrences of  the words matter a lot in the classification problem. Such an example is when we try perform topic classification. </span></li><li><span class="name">The Binarized Multinomial Naive Bayes is used when the frequencies of the words don't play a key role in our classification. Such an example is sentimental Analysis, where it does not really matters how many times someone mentions the word "bad" but rather only the fact that he does. </span></li><li><span class="name">Bernoulli Naive Bayes can be used when in our problem the absence of a particular word matters. For example Bernoulli commonly used in spam or Adult Content Detection with very good results</span></li></ul></li><li><span class="name">Naive Bayes classifier assumes that the features used in the classification are independent. Despite it can be proven that even  though the probability estimates of Naive Bayes are of low quality, its classification decision are quite good. Thus, despite the fact that Naive Bayes usually over estimates the probability of the selected class, given that we use it only too make the decision and not to accurately predict the actual probability. </span></li><li><span class="name">Due to the fact that computers can handle numbers with specific decimals point accuracy, calculating the product of the above probabilities will lead to float point underflow. This means that we will end up with a number so small, that will not be able to fit in memory and thus it will be rounded to zero,rendering our analysis useless. To avoid this instead of maximizing the product of the probabilities we will maximize the sum of their logarithms.</span></li></ul></li></ul></li></ul></body></html>