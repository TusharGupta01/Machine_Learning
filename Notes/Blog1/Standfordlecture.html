<!doctype html><html><head><meta charset="utf-8"><style>
      /* Separated from document_view.css to enable use in downloadable
   export files. #cutAndCopyBucket is included for every rule because
   we wwant cut/copied items to be formatted the same way as they
   would be when exported. */

.formattedExport,
#cutAndCopyBucket {
    font-family: 'Helvetica Neue', Arial, Sans-serif;
    color: #333;
    font-size: 13px;
    line-height: 17px;
    /* used to set white-space: pre-wrap here, but that messes up GMail's
       formatting of the export output, so moving that to the name and
       notes separately. */
}

.formattedExport .name, .formattedExport .note,
#cutAndCopyBucket .name, #cutAndCopyBucket .note {
    white-space: pre-wrap;
}

.formattedExport ul,
#cutAndCopyBucket ul{
    list-style: disc;
    /* Needed to reset browser defaults. */
    margin: 0;
    padding: 0;
}

.formattedExport li,
#cutAndCopyBucket li {
    margin: 4px 0 4px 20px;
    /* Needed to reset browser defaults. */
    padding: 0;
}

.formattedExport > .name,
#cutAndCopyBucket > .name {
    font-size: 16px;
    line-height: 21px;
}

.formattedExport > .note,
#cutAndCopyBucket > .note {
    font-size: 13px;
    line-height: 17px;
}

.formattedExport > ul,
#cutAndCopyBucket > ul {
    margin-top: 15px;
}

.formattedExport .name.done,
#cutAndCopyBucket .name.done {
    text-decoration:line-through;
    color:#999;
}

.formattedExport .note,
#cutAndCopyBucket .note {
    font-size: 12px;
    color:#666;
}

      </style></head><body class="formattedExport"><span class="name">Standford course</span><ul><li><span class="name">Gradient Descent Algorithm</span><ul><li><span class="name">Used to minimize some cost function.  </span></li><li><span class="name">Algorithm</span><ul><li><span class="name">Start with some /theta_0, /theta_1.</span></li><li><span class="name">Keep changing  /theta_0, /theta_1 to reduce J( /theta_0, /theta_1) until we hopefully end up at a minimum. </span></li><li><span class="name">You may end up with different local minimum, depending on the point you choose initially.  </span></li></ul></li><li><span class="name">alpha: is learning rate and denotes how big steps are we taking(positive number). </span><ul><li><span class="name">If alpha is too small, gradient descent can be slow.</span></li><li><span class="name">If alpha is too large, gradient descent can overshoot the minimum. It may fail to converge, or even diverge. </span></li></ul></li><li><span class="name">If we are local minimum than the derivative term will get 0 and the equation will get converge. </span></li><li><span class="name">Gradient descent can converge to a local minimum, even with the learning rate alpha fixed.</span></li><li><span class="name">As we approach a local minimum, gradient descent will automatically take smaller steps. So, mo need to decrease alpha over time.  </span></li><li><span class="name">Batch: Each step of gradient descent uses all the training examples. </span></li><li><span class="name">How to choose alpha ?</span><ul><li><span class="name">For sufficient small alpha, J(/theta) should decrease on every iteration. </span></li><li><span class="name">But if alpha is too small, gradient descent can be slow to converge.</span></li><li><span class="name">If alpha is too large: J(/theta) may not decrease on every iteration; may not converge. (slow convergence is also possible)</span></li><li><span class="name">Try to choose ........,0.001,0.003,0.01,0.03,0.1,0.3,1, ...... 3 fold increase.  </span></li></ul></li><li><span class="name">J(/theta) should decrease after every iterations.</span></li></ul></li><li><span class="name">Normal Equation</span><ul><li><span class="name">Method to solve /theta analytically.</span></li><li><span class="name"></span></li></ul></li></ul></body></html>