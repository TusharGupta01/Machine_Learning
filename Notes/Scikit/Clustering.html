<!doctype html><html><head><meta charset="utf-8"><style>
      /* Separated from document_view.css to enable use in downloadable
   export files. #cutAndCopyBucket is included for every rule because
   we wwant cut/copied items to be formatted the same way as they
   would be when exported. */

.formattedExport,
#cutAndCopyBucket {
    font-family: 'Helvetica Neue', Arial, Sans-serif;
    color: #333;
    font-size: 13px;
    line-height: 17px;
    /* used to set white-space: pre-wrap here, but that messes up GMail's
       formatting of the export output, so moving that to the name and
       notes separately. */
}

.formattedExport .name, .formattedExport .note,
#cutAndCopyBucket .name, #cutAndCopyBucket .note {
    white-space: pre-wrap;
}

.formattedExport ul,
#cutAndCopyBucket ul{
    list-style: disc;
    /* Needed to reset browser defaults. */
    margin: 0;
    padding: 0;
}

.formattedExport li,
#cutAndCopyBucket li {
    margin: 4px 0 4px 20px;
    /* Needed to reset browser defaults. */
    padding: 0;
}

.formattedExport > .name,
#cutAndCopyBucket > .name {
    font-size: 16px;
    line-height: 21px;
}

.formattedExport > .note,
#cutAndCopyBucket > .note {
    font-size: 13px;
    line-height: 17px;
}

.formattedExport > ul,
#cutAndCopyBucket > ul {
    margin-top: 15px;
}

.formattedExport .name.done,
#cutAndCopyBucket .name.done {
    text-decoration:line-through;
    color:#999;
}

.formattedExport .note,
#cutAndCopyBucket .note {
    font-size: 12px;
    color:#666;
}

      </style></head><body class="formattedExport"><span class="name">Scikit Learn</span><ul><li><span class="name">Clustering</span><ul><li><span class="name">Kmeans or Lloyd's algorithm</span><ul><li><span class="name">The Kmeans algorithm data by trying to separate samples in n group of equal variance, minimizing a criterion known as the <span class="contentBold">inertia </span>or within-cluster sum-of-squares.</span></li><li><span class="name"> This algorithm requires the number of clusters to be specified.</span></li><li><span class="name">The K-means algorithm divides a set of N samples X into K disjoints clusters C, each described by the mean /mu_j of the samples in the cluster. The means are commonly called the cluster "centroids".</span></li><li><span class="name"> The K-means algorithm aims to choose centroids that <span class="contentBold">minimize the inertia</span>, or within-cluster sum of squared criterion:</span></li><li><span class="name">Inertia </span><ul><li><span class="name">Assume that clusters are convex and isotropic, therefore it responds poorly to elongated clusters, or manifolds with irregular shapes. </span></li><li><span class="name">Inertia is not a normalized metric. </span></li></ul></li><li><span class="name">Voronoi diagram</span><ul><li><span class="name">In mathematics, a Voronoi diagram  is a partitioning of a plane into regions based on the distance to points in a specific subset of the plane. </span></li><li><span class="name">Kmeans clustering can be understood by Voronoi diagram(relate it).</span></li></ul></li><li><span class="name">K-means will always converge , however this may be to a local minimum. This is highly d<span class="contentBold">ependent on the initialization of the centroids</span>.</span></li><li><span class="name">Good initialization of centroids. </span><ul><li><span class="name">Initialize the centroids to be distant from each other, leading to provably better results than random initialization.</span></li></ul></li><li><span class="name">Complexity</span><ul><li><span class="name">The average complexity is given by O(K n T), where n is the number of samples and T is the number of iterations. </span></li><li><span class="name">In practice, the K-means algorithm is very fast. But the worst case time complexity is given by 0(n^(k+2/p)) with n = n_samples, p = n_features.</span></li></ul></li></ul></li><li><span class="name">Affinity Propagation</span><ul><li><span class="name">Affinity Propagation creates clusters by sending messages between pairs of samples until convergence. </span></li><li><span class="name">Affinity Propagation can be interesting as it chooses the number of clusters based on the data provided. </span></li><li><span class="name">The two important parameters are the preferences, which controls how many exemplars are used, and the damping factor. </span></li><li><span class="name">Drawback</span><ul><li><span class="name">The main drawback of Affinity Propagation is its complexity. The algorithm has time complexity of the order O(N^2 T), where N is number of samples and T is number of iterations until convergence. </span></li><li><span class="name">The memory complexity is of the order O(N^2).</span></li></ul></li><li><span class="name">Algorithm</span><ul><li><span class="name">The messages sent between points belong to one of two categories. The first is the responsibility r(i,k), which is the accumulated evidence that sample k should be the exemplar for sample i. The second is the availability a(i,k) which is the accumulated evidence that sample i should choose sample k to be its exemplar, and consider the values for all other sample that k should be an exemplar. </span></li></ul></li></ul></li><li><span class="name">Mean Shift </span><ul><li><span class="name">Mean Shift clustering aims to discover blobs in a smooth density of samples. It is a centroid based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids. </span></li><li><span class="name">The algorithm automatically sets the number of clusters, instead of relying on a parameter bandwidth, which dictates the size of the region to search through. There is estimate function in scikit learn: <span class="contentBold">estimate_bandwidth. </span></span></li><li><span class="name">The algorithm is not highly scalable, as it requires multiple nearest neighbor searches during the execution of the algorithm. The algorithm will stop iterating when the change in centroids is small. </span></li><li><span class="name">It's implementation uses a flat kernel and a ball tree to look up members of each kernel.</span></li><li><span class="name"> The complexity is O(T*n*log(n)) in lower dimensions, with n the number of samples and T the number of points. In higher dimensions the complexity will tend towards O(T*n^2).</span></li></ul></li></ul></li></ul></body></html>