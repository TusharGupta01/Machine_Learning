<!doctype html><html><head><meta charset="utf-8"><style>
      /* Separated from document_view.css to enable use in downloadable
   export files. #cutAndCopyBucket is included for every rule because
   we wwant cut/copied items to be formatted the same way as they
   would be when exported. */

.formattedExport,
#cutAndCopyBucket {
    font-family: 'Helvetica Neue', Arial, Sans-serif;
    color: #333;
    font-size: 13px;
    line-height: 17px;
    /* used to set white-space: pre-wrap here, but that messes up GMail's
       formatting of the export output, so moving that to the name and
       notes separately. */
}

.formattedExport .name, .formattedExport .note,
#cutAndCopyBucket .name, #cutAndCopyBucket .note {
    white-space: pre-wrap;
}

.formattedExport ul,
#cutAndCopyBucket ul{
    list-style: disc;
    /* Needed to reset browser defaults. */
    margin: 0;
    padding: 0;
}

.formattedExport li,
#cutAndCopyBucket li {
    margin: 4px 0 4px 20px;
    /* Needed to reset browser defaults. */
    padding: 0;
}

.formattedExport > .name,
#cutAndCopyBucket > .name {
    font-size: 16px;
    line-height: 21px;
}

.formattedExport > .note,
#cutAndCopyBucket > .note {
    font-size: 13px;
    line-height: 17px;
}

.formattedExport > ul,
#cutAndCopyBucket > ul {
    margin-top: 15px;
}

.formattedExport .name.done,
#cutAndCopyBucket .name.done {
    text-decoration:line-through;
    color:#999;
}

.formattedExport .note,
#cutAndCopyBucket .note {
    font-size: 12px;
    color:#666;
}

      </style></head><body class="formattedExport"><span class="name">Scikit Learn</span><ul><li><span class="name">Clustering</span><ul><li><span class="name">Kmeans or Lloyd's algorithm</span><ul><li><span class="name">The Kmeans algorithm data by trying to separate samples in n group of equal variance, minimizing a criterion known as the <span class="contentBold">inertia </span>or within-cluster sum-of-squares.</span></li><li><span class="name"> This algorithm requires the number of clusters to be specified.</span></li><li><span class="name">The K-means algorithm divides a set of N samples X into K disjoints clusters C, each described by the mean /mu_j of the samples in the cluster. The means are commonly called the cluster "centroids".</span></li><li><span class="name"> The K-means algorithm aims to choose centroids that <span class="contentBold">minimize the inertia</span>, or within-cluster sum of squared criterion:</span></li><li><span class="name">Inertia </span><ul><li><span class="name">Assume that clusters are convex and isotropic, therefore it responds poorly to elongated clusters, or manifolds with irregular shapes. </span></li><li><span class="name">Inertia is not a normalized metric. </span></li></ul></li><li><span class="name">Voronoi diagram</span><ul><li><span class="name">In mathematics, a Voronoi diagram  is a partitioning of a plane into regions based on the distance to points in a specific subset of the plane. </span></li><li><span class="name">Kmeans clustering can be understood by Voronoi diagram(relate it).</span></li></ul></li><li><span class="name">K-means will always converge , however this may be to a local minimum. This is highly d<span class="contentBold">ependent on the initialization of the centroids</span>.</span></li><li><span class="name">Good initialization of centroids. </span><ul><li><span class="name">Initialize the centroids to be distant from each other, leading to provably better results than random initialization.</span></li></ul></li><li><span class="name">Complexity</span><ul><li><span class="name">The average complexity is given by O(K n T), where n is the number of samples and T is the number of iterations. </span></li><li><span class="name">In practice, the K-means algorithm is very fast. But the worst case time complexity is given by 0(n^(k+2/p)) with n = n_samples, p = n_features.</span></li></ul></li></ul></li><li><span class="name">Affinity Propagation</span><ul><li><span class="name">Affinity Propagation creates clusters by sending messages between pairs of samples until convergence. </span></li><li><span class="name">Affinity Propagation can be interesting as it chooses the number of clusters based on the data provided. </span></li><li><span class="name">The two important parameters are the preferences, which controls how many exemplars are used, and the damping factor. </span></li><li><span class="name">Drawback</span><ul><li><span class="name">The main drawback of Affinity Propagation is its complexity. The algorithm has time complexity of the order O(N^2 T), where N is number of samples and T is number of iterations until convergence. </span></li><li><span class="name">The memory complexity is of the order O(N^2).</span></li></ul></li><li><span class="name">Algorithm</span><ul><li><span class="name">The messages sent between points belong to one of two categories. The first is the responsibility r(i,k), which is the accumulated evidence that sample k should be the exemplar for sample i. The second is the availability a(i,k) which is the accumulated evidence that sample i should choose sample k to be its exemplar, and consider the values for all other sample that k should be an exemplar. </span></li></ul></li></ul></li></ul></li></ul></body></html>